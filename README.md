# Description
The respositiy is public package of the paper titled "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues" submitted to ACL 2024.

Puzzler Architecture:  
![Image text](https://github.com/czycurefun/IJBR/blob/main/fig/final_artifactureV2.0.png)
- OMG.py is to generate offensive strategies  
- jailbreak.py is to jailbreak LLM

Updated Result:  
![Image text](https://github.com/czycurefun/IJBR/blob/main/fig/new_result.jpg)

# Experiment Setup  
We maintained the default configuration of GPT-3.5, GPT-4, and GPT-4 Turbo with temperature = 1 and top_n = 1

# Run the code
python OMG.py  
python jailbreak.py  







